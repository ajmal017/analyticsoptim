{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2014-01-01\"\n",
    "end = \"2020-08-20\"\n",
    "conn = psycopg2.connect(host=\"slave\",\n",
    "                        database=\"testdb1\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"reallyStrongPwd123\")\n",
    "cursor = conn.cursor()\n",
    "symbols = (\"MSFT\", \"GOOGL\", \"FB\")\n",
    "query2 = f\"\"\" SELECT * from incertae.\"StockData\" where symbol in {symbols} and date between '{start}' AND '{end}' \"\"\"\n",
    "df = pd.read_sql_query(query2, con=conn)\n",
    "df.set_index([\"date\", \"symbol\"], inplace=True)\n",
    "df = df.unstack()\n",
    "df.columns = df.columns.values\n",
    "y= df[('high', 'FB')].shift(1)\n",
    "x=df.drop(('high', 'FB'),axis=1)\n",
    "y = y.fillna(0)\n",
    "\n",
    "N, D_in, D_out = 100, df.shape[1]-1, 1\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.linear(X)\n",
    "        return out\n",
    "model = LinearRegression(D_in, D_out)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "for epoch in range(100):\n",
    "    inputs = Variable(torch.from_numpy( x.values)).float()\n",
    "    labels = Variable(torch.from_numpy( y.values)).float()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1671, 17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predicted = model(Variable(torch.from_numpy(x.values)).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        ...,\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predicted = model(Variable(torch.from_numpy(x.values)).float())\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1586e+14, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(128.7580, grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 128.7580108642578\n",
      "tensor(10.5120, grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 10.5120267868042\n",
      "tensor(0.8670, grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 0.8669695258140564\n",
      "tensor(0.0801, grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.08014492690563202\n",
      "tensor(0.0159, grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.015860892832279205\n",
      "tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.010513232089579105\n",
      "tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.00997408851981163\n",
      "tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.009828353300690651\n",
      "tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.009715796448290348\n",
      "tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.009606998413801193\n",
      "tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.009499729610979557\n",
      "tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.009393606334924698\n",
      "tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.00928875058889389\n",
      "tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.009185021743178368\n",
      "tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.00908241979777813\n",
      "tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.008981051854789257\n",
      "tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.00888073816895485\n",
      "tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.008781605400145054\n",
      "tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.008683527819812298\n",
      "tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.008586570620536804\n",
      "tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.00849066860973835\n",
      "tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.008395860902965069\n",
      "tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.008302085101604462\n",
      "tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.00820935983210802\n",
      "tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.00811772234737873\n",
      "tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.008027071133255959\n",
      "tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.007937457412481308\n",
      "tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.007848788052797318\n",
      "tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.007761133834719658\n",
      "tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.007674464490264654\n",
      "tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.0075887637212872505\n",
      "tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.007504050619900227\n",
      "tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.0074202255345880985\n",
      "tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.007337347138673067\n",
      "tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.007255473639816046\n",
      "tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.0071744234301149845\n",
      "tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.00709434924647212\n",
      "tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.0070151062682271\n",
      "tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.006936777848750353\n",
      "tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.006859269458800554\n",
      "tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.006782672833651304\n",
      "tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.006706990767270327\n",
      "tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.006632054224610329\n",
      "tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.0065579949878156185\n",
      "tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.006484786048531532\n",
      "tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.006412321235984564\n",
      "tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.006340759806334972\n",
      "tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.00626993365585804\n",
      "tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.00619991822168231\n",
      "tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.00613071583211422\n",
      "tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.0060622673481702805\n",
      "tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.005994541570544243\n",
      "tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.005927566904574633\n",
      "tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.005861407611519098\n",
      "tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.005795991513878107\n",
      "tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.005731245968490839\n",
      "tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.005667222198098898\n",
      "tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.005603942088782787\n",
      "tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.005541377700865269\n",
      "tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.0054795038886368275\n",
      "tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.005418301094323397\n",
      "tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.005357787478715181\n",
      "tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.005297947209328413\n",
      "tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.005238817539066076\n",
      "tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.005180298816412687\n",
      "tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.0051224688068032265\n",
      "tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.005065246485173702\n",
      "tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.0050087179988622665\n",
      "tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.004952752031385899\n",
      "tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.00489745894446969\n",
      "tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.004842773545533419\n",
      "tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.004788689780980349\n",
      "tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.004735197406262159\n",
      "tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.004682349041104317\n",
      "tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.004630046896636486\n",
      "tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.0045783319510519505\n",
      "tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.004527213051915169\n",
      "tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.004476678557693958\n",
      "tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.004426665138453245\n",
      "tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.004377223085612059\n",
      "tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.004328374285250902\n",
      "tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.00428001768887043\n",
      "tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.004232227336615324\n",
      "tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.00418495386838913\n",
      "tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.004138266667723656\n",
      "tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.004092030227184296\n",
      "tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.004046339076012373\n",
      "tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.004001127555966377\n",
      "tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.00395646458491683\n",
      "tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.0039122807793319225\n",
      "tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.003868628991767764\n",
      "tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.0038253848906606436\n",
      "tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.003782691666856408\n",
      "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.00374044943600893\n",
      "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.003698707325384021\n",
      "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.0036573901306837797\n",
      "tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.0036165579222142696\n",
      "tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.003576137125492096\n",
      "tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.003536197356879711\n",
      "tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.0034967265091836452\n"
     ]
    }
   ],
   "source": [
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
